{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d3f1a5-4b86-4423-a683-895c9b41d2e3",
   "metadata": {},
   "source": [
    "# How to build a recommendation system using Tensorflow ranking?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608126f-6d3e-44cd-87ca-c3273517c008",
   "metadata": {},
   "source": [
    "**The Tensorflow library is an implementaio from Tensorflow that helps us in building learning-to-ranking (LTR) models. The learning to rank (LTR) models that helps us in constructing  the ranking models for any information retrieval system.**\n",
    "\n",
    "\n",
    "Machine learning  ranking us an approach to build scalable information retrieval  system especially  when the task is to find out the similar items for a givine  input  value. Recommender systems  also find and present similar items based on several characteristics. TensorFlow Ranking  is a Python library  that helps in building learning  to ranking machine learning models. In this article, we will discuss how we ca use Tensorflow ranking to build a recommendattion system  based on the learning-to-rank concept. The demonstration here is inspired by the Tensorflow tutorials on Tensorflow  ranking. The major to be discused in the article are listed below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387266d3-f579-483d-9ef4-f5dd72fdeb01",
   "metadata": {},
   "source": [
    "## What is Tensorflow ranking?\n",
    "\n",
    "The Tensorflow ranking is an implementaion from Tensorflow that helps us in  building  learning-to-rank(LTR) models. The learning to rank(LTR) models are models  that help us in constructing the ranking data with a list of items and these items are connected with some partial orders. Representaion of partial order can be a numerical score or a binary judgment.\n",
    "\n",
    "The main purpose of this type of model is to predict the rank of new items similar to the training data. Using the Tensorflow ranking we can produce such  models. Also, these models find their uses in various tasks such  as collaborative filtering, sentiment analysis, and personalized advertisement. A possible architecture of such models can be explained by the following figure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c62509c-0127-4ef1-a96b-844f076f8480",
   "metadata": {},
   "source": [
    "This implementation also provides various modules to speed up the building od LTR models wherein the backgroud these modules work on the Karas API. Since the LTR models have theie applications in generating recommendation systems. In this article, we are going to use the tensorflow ranking for making a recommendation system. Before staring the procedure we are reuired to install  this implementation that can be done using the following line of codes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649000ca-d868-4d47-ad56-0d6739c9c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaa8fd0-98d9-4711-adef-505ea6579c70",
   "metadata": {},
   "source": [
    "After installation, we are ready to implemnet it in our work.\n",
    "\n",
    "### Building recommendation  system.\n",
    "\n",
    "In this article, we are going to make  a recommendation  system  using the Tensorflow ranking packages, so that we can utilze the model to rank the movies and then  recommadation them to user.\n",
    "\n",
    "### Improting and preprocessing  data.\n",
    "\n",
    "Here in the article, we are going to use the movielens dataset for making recommndation systems that can be called from the tensorflow_dataset modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923b65b1-4e60-4b8f-9094-91f2e714c94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\LAMSD\\tensorflow_datasets\\movielens\\100k-movies\\0.1.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfaee2fad184f1da8ed470cac6c9a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11cb3635cacb4eda95f6b9d3a9cbdf1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66914cbe0f00457ba6330c94f320125f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling movielens-train.tfrecord...:   0%|          | 0/1682 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset movielens downloaded and prepared to C:\\Users\\LAMSD\\tensorflow_datasets\\movielens\\100k-movies\\0.1.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "ratings_data = tfds.load('movielens/100k-ratings', split=\"train\")\n",
    "fetures_data = tfds.load('movielens/100k-movies', split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ac311c-eb20-4e2f-96d3-44377863a7eb",
   "metadata": {},
   "source": [
    "Selecting the features from the rating data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "584f98f0-3706-4fb3-88ef-d5024708ee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data = ratings_data.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"user_rating\": x[\"user_rating\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7679a8-ce9d-4677-889e-29d94efa6fba",
   "metadata": {},
   "source": [
    "Converting iser_ids and movie_title into integer indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a818fa6d-28de-4368-9378-b74d2d80aaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers \n",
    "feature_data = fetures_data.map(lambda x: x[\"movie_title\"])\n",
    "users = ratings_data.map(lambda x: x[\"user_id\"])\n",
    " \n",
    "user_ids_vocabulary = layers.experimental.preprocessing.StringLookup(mask_token=None)\n",
    "user_ids_vocabulary.adapt(users.batch(1000))\n",
    " \n",
    "movie_titles_vocabulary = layers.experimental.preprocessing.StringLookup(mask_token=None)\n",
    "movie_titles_vocabulary.adapt(feature_data.batch(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270c34ac-3ab5-4b2b-b9cf-2c0debc665d3",
   "metadata": {},
   "source": [
    "Group by user_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b3a7e8d-9746-4812-8c28-87f31c6567ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_func = lambda x: user_ids_vocabulary(x[\"user_id\"])\n",
    "reduce_func = lambda key, dataset: dataset.batch(100)\n",
    "train = ratings_data.group_by_window(key_func=key_func, reduce_func=reduce_func, window_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6981e96-63eb-4fd1-af22-334bc280aa26",
   "metadata": {},
   "source": [
    "Here we can check the shape of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b79ce31e-00cb-4dbe-adfc-be2f0e5fcc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_GroupByWindowDataset element_spec={'movie_title': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'user_id': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'user_rating': TensorSpec(shape=(None,), dtype=tf.float32, name=None)}>\n",
      "Shape of movie_title: (100,)\n",
      "Example values of movie_title: [b'Man Who Would Be King, The (1975)' b'Silence of the Lambs, The (1991)'\n",
      " b'Next Karate Kid, The (1994)' b'2001: A Space Odyssey (1968)'\n",
      " b'Usual Suspects, The (1995)']\n",
      "\n",
      "Shape of user_id: (100,)\n",
      "Example values of user_id: [b'405' b'405' b'405' b'405' b'405']\n",
      "\n",
      "Shape of user_rating: (100,)\n",
      "Example values of user_rating: [1. 4. 1. 5. 5.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train)\n",
    "for x in train.take(1):\n",
    "    for key, value in x.items():\n",
    "        print(f\"Shape of {key}: {value.shape}\")\n",
    "        print(f\"Example values of {key}: {value[:5].numpy()}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a099dd27-3981-4f9f-8e8b-cb081b1a14fc",
   "metadata": {},
   "source": [
    "Generating batch of labels and features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "223b25bf-e5e8-4c9d-8f4d-dd7bd45536e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "def _features_and_labels(\n",
    "    x: Dict[str, tf.Tensor]) -> Tuple[Dict[str, tf.Tensor], tf.Tensor]:\n",
    "  labels = x.pop(\"user_rating\")\n",
    "  return x, labels\n",
    " \n",
    " \n",
    "train = train.map(_features_and_labels)\n",
    " \n",
    "train = train.apply(\n",
    "    tf.data.experimental.dense_to_ragged_batch(batch_size=32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a7b8f-7a76-4fe1-8845-df4165a45589",
   "metadata": {},
   "source": [
    "Here in the above codes, we have tensor of user id and movie titles in the train of shape [32, none]. Let’s define the model.\n",
    "\n",
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1440f22b-f705-499e-ab91-430138756af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "class RankingModel(Model):\n",
    " \n",
    "  def __init__(self, user_vocab, movie_vocab):\n",
    "    super().__init__()\n",
    "    self.user_vocab = user_vocab\n",
    "    self.movie_vocab = movie_vocab\n",
    "    self.user_embed = layers.Embedding(user_vocab.vocabulary_size(),\n",
    "                                                64)\n",
    "    self.movie_embed = layers.Embedding(movie_vocab.vocabulary_size(),\n",
    "                                                 64)\n",
    " \n",
    "  def call(self, features: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    " \n",
    "    embeddings_user= self.user_embed(self.user_vocab(features[\"user_id\"]))\n",
    "    embeddings_movie = self.movie_embed(\n",
    "        self.movie_vocab(features[\"movie_title\"]))\n",
    " \n",
    "    return tf.reduce_sum(embeddings_user * embeddings_movie, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86247702-a82f-4897-a4b1-c8bb662fe68d",
   "metadata": {},
   "source": [
    "Here in the above codes, we have defined a class in which we defined a function to set the user and movie vocabulary and embeddings and a call function to define how the ranking will be calculated.  In the outcome, we will be having dot products of user embeddings and movie embeddings.\n",
    "\n",
    "## Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e1d9c35-efc8-400a-9b49-56ccc7a9dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_ranking as tfr\n",
    "from tensorflow.keras import optimizers\n",
    "model = RankingModel(user_ids_vocabulary, movie_titles_vocabulary)\n",
    "optimizer = optimizers.Adagrad(0.5)\n",
    "loss = tfr.keras.losses.get(\n",
    "    loss=tfr.keras.losses.RankingLossKey.SOFTMAX_LOSS, ragged=True)\n",
    "eval_metrics = [\n",
    "    tfr.keras.metrics.get(key=\"ndcg\", name=\"metric/ndcg\", ragged=True),\n",
    "    tfr.keras.metrics.get(key=\"mrr\", name=\"metric/mrr\", ragged=True)\n",
    "]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5290d2e1-0fd9-4662-97d3-7ac190f55340",
   "metadata": {},
   "source": [
    "In the above, we have used the ranking loss for the training of the model and ranking metrics for the evaluation of the model from the TensorFlow ranking package. Also from ranking metrics, we specified the normalized discounted cumulative gain and mean reciprocal rank. \n",
    "\n",
    "## Model fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f96e975-05a9-4e84-9f07-4522357e61b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAMSD\\my_app\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/RaggedToTensor_2/boolean_mask_1/GatherV2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/RaggedToTensor_2/boolean_mask/GatherV2:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"gradient_tape/RaggedToTensor_2/Shape:0\", shape=(1,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\LAMSD\\my_app\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/ranking_model/RaggedTile/Reshape_3:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/ranking_model/RaggedTile/Reshape_2:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/ranking_model/RaggedTile/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "C:\\Users\\LAMSD\\my_app\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/ranking_model/RaggedTile_1/Reshape_3:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/ranking_model/RaggedTile_1/Reshape_2:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/ranking_model/RaggedTile_1/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 6s 38ms/step - loss: 998.7497 - metric/ndcg: 0.8262 - metric/mrr: 1.0000\n",
      "Epoch 2/9\n",
      "48/48 [==============================] - 2s 39ms/step - loss: 997.0566 - metric/ndcg: 0.9174 - metric/mrr: 1.0000\n",
      "Epoch 3/9\n",
      "48/48 [==============================] - 2s 39ms/step - loss: 994.8642 - metric/ndcg: 0.9394 - metric/mrr: 1.0000\n",
      "Epoch 4/9\n",
      "48/48 [==============================] - 2s 40ms/step - loss: 992.9920 - metric/ndcg: 0.9588 - metric/mrr: 1.0000\n",
      "Epoch 5/9\n",
      "48/48 [==============================] - 2s 40ms/step - loss: 991.5983 - metric/ndcg: 0.9707 - metric/mrr: 1.0000\n",
      "Epoch 6/9\n",
      "48/48 [==============================] - 2s 38ms/step - loss: 990.6997 - metric/ndcg: 0.9784 - metric/mrr: 1.0000\n",
      "Epoch 7/9\n",
      "48/48 [==============================] - 2s 40ms/step - loss: 990.1334 - metric/ndcg: 0.9831 - metric/mrr: 1.0000\n",
      "Epoch 8/9\n",
      "48/48 [==============================] - 2s 40ms/step - loss: 989.7781 - metric/ndcg: 0.9862 - metric/mrr: 1.0000\n",
      "Epoch 9/9\n",
      "48/48 [==============================] - 2s 39ms/step - loss: 989.5406 - metric/ndcg: 0.9881 - metric/mrr: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train, epochs=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a47fae3-ce5f-403a-b13a-48294c5ada5b",
   "metadata": {},
   "source": [
    "In the above, we have trained our compiled model on the data using 9 epochs. Let’s check the history of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5a61fd0-f2eb-4b6a-88bc-2a3c0f14d903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [998.7496948242188,\n",
       "  997.056640625,\n",
       "  994.8641967773438,\n",
       "  992.9920043945312,\n",
       "  991.5983276367188,\n",
       "  990.69970703125,\n",
       "  990.1333618164062,\n",
       "  989.7781372070312,\n",
       "  989.5406494140625],\n",
       " 'metric/ndcg': [0.8262100219726562,\n",
       "  0.9173583388328552,\n",
       "  0.9394474625587463,\n",
       "  0.958753228187561,\n",
       "  0.9707444906234741,\n",
       "  0.9784186482429504,\n",
       "  0.9830535650253296,\n",
       "  0.9862099885940552,\n",
       "  0.9881227016448975],\n",
       " 'metric/mrr': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5fe80c-6b6e-4aca-936e-fe838704f33d",
   "metadata": {},
   "source": [
    "In history, we can see that loss from the model is so high because we are using ranking-specific softmax loss that is different from the softmax loss in classification problems. This loss promotes all relevant items in the ranking list that have better chances than irrelevant items. \n",
    "\n",
    "## Generating prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c30b651d-2f57-416c-8ddc-8a94670c2991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 recommendations for user 26: [b\"Schindler's List (1993)\" b'L.A. Confidential (1997)' b'Titanic (1997)'\n",
      " b'Shawshank Redemption, The (1994)' b'Alien (1979)'\n",
      " b'Seven (Se7en) (1995)' b'Private Parts (1997)' b'Cool Hand Luke (1967)'\n",
      " b'M*A*S*H (1970)' b'Great Escape, The (1963)']\n"
     ]
    }
   ],
   "source": [
    "for movie_titles in feature_data.batch(2000):\n",
    "  break\n",
    " \n",
    "inputs = {\n",
    "    \"user_id\":\n",
    "        tf.expand_dims(tf.repeat(\"26\", repeats=movie_titles.shape[0]), axis=0),\n",
    "    \"movie_title\":\n",
    "        tf.expand_dims(movie_titles, axis=0)\n",
    "}\n",
    " \n",
    "scores = model(inputs)\n",
    "titles = tfr.utils.sort_by_scores(scores,\n",
    "                                  [tf.expand_dims(movie_titles, axis=0)])[0]\n",
    "print(f\"Top 10 recommendations for user 26: {titles[0, :10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a095bd-547e-4a99-90e0-3007f0980e3c",
   "metadata": {},
   "source": [
    "Here in the above codes, we have created a list of users and movies from which we have generated an input as user number 26. Using the input and scores model has recommended 10 movie names for user 26.\n",
    "\n",
    "## Final words\n",
    "\n",
    "In this article, we have discussed the TensorFlow ranking that is an implementation from TensorFlow for learning to rank modelling. Using this module we have generated a recommendation system on movielens dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe36a74b-2cf9-4958-947d-594fa4b4e86a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
